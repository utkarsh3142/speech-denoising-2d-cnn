{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 - Speech Denoising Using 2D CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "onpCATiIxrHH"
   },
   "outputs": [],
   "source": [
    "import librosa \n",
    "import numpy as np\n",
    "\n",
    "# Reading the training file\n",
    "s, sr=librosa.load('data/train_clean_male.wav', sr=None) \n",
    "S_input=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "\n",
    "sn, sr=librosa.load('data/train_dirty_male.wav', sr=None) \n",
    "X_input=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "\n",
    "# Reading test 01 file\n",
    "st_01, sr_01=librosa.load('data/test_x_01.wav', sr=None) \n",
    "X_test_01=librosa.stft(st_01, n_fft=1024, hop_length=512)\n",
    "\n",
    "# Reading test 02 file\n",
    "st_02, sr_02=librosa.load('data/test_x_02.wav', sr=None) \n",
    "X_test_02=librosa.stft(st_02, n_fft=1024, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gWIAsYWxrHL"
   },
   "outputs": [],
   "source": [
    "# Get the magnitudes of training set, and test sets\n",
    "S_mag = np.abs(S_input).T\n",
    "X_mag = np.abs(X_input).T\n",
    "X_test_01_mag = np.abs(X_test_01).T\n",
    "X_test_02_mag = np.abs(X_test_02).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "7TygE6DyxrHM",
    "outputId": "0255b727-7fa6-4c5c-c92b-8d26e62ec944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean training audio shape (S_mag):  (2459, 513)\n",
      "Noisy training audio shape (X_mag):  (2459, 513)\n",
      "First test audio shape (X_test_01_mag):  (142, 513)\n",
      "Second test audio shape (X_test_02_mag):  (380, 513)\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean training audio shape (S_mag): \",S_mag.shape)\n",
    "print(\"Noisy training audio shape (X_mag): \",X_mag.shape)\n",
    "print(\"First test audio shape (X_test_01_mag): \",X_test_01_mag.shape)\n",
    "print(\"Second test audio shape (X_test_02_mag): \",X_test_02_mag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fE7siriUxrHP"
   },
   "outputs": [],
   "source": [
    "def snr(dirty, clean):\n",
    "    return round(10 * np.log10(np.sum(np.square(clean))/np.sum(np.square(clean - dirty))),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "y3bdZCRIxrHQ",
    "outputId": "4b77a12a-4afa-47df-b6b7-54e31b4701bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noisy training audio shape for 2d CNN (X_mag_train):  (2440, 20, 513, 1)\n",
      "First test audio shape for 2d CNN(X_test1):  (123, 20, 513, 1)\n",
      "Second test audio shape for 2d CNN(X_test2):  (361, 20, 513, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import newaxis\n",
    "\n",
    "# Create data for 2d CNN by taking frames of 20x513 from the data \n",
    "X_mag_train = []\n",
    "for i in range(len(X_mag)-19):\n",
    "    X_mag_train.append(X_mag[i:i+20,:])\n",
    "    \n",
    "X_mag_train = np.array(X_mag_train)[..., newaxis]\n",
    "\n",
    "X_test1 = []\n",
    "for i in range(len(X_test_01_mag)-19):\n",
    "    X_test1.append(X_test_01_mag[i:i+20,:])\n",
    "    \n",
    "X_test1 = np.array(X_test1)[..., newaxis]\n",
    "\n",
    "X_test2 = []\n",
    "for i in range(len(X_test_02_mag)-19):\n",
    "    X_test2.append(X_test_02_mag[i:i+20,:])\n",
    "    \n",
    "X_test2 = np.array(X_test2)[..., newaxis]\n",
    "\n",
    "# For cleaned up signal and test signals, cliping the sound at the beggining - removing first 19 values\n",
    "S_mag_train = S_mag[19:]\n",
    "\n",
    "print(\"Noisy training audio shape for 2d CNN (X_mag_train): \",X_mag_train.shape)\n",
    "print(\"First test audio shape for 2d CNN(X_test1): \",X_test1.shape)\n",
    "print(\"Second test audio shape for 2d CNN(X_test2): \",X_test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eg-CJ0CrxrHS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\utkar\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\utkar\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create placeholders - convention used - [batch_size, height, width, num_channels]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 20, 513, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 513])\n",
    "\n",
    "# Create filters - [width, height, num_channels_in, num_channels_out]\n",
    "W1 = tf.get_variable(\"W1\", [2, 2, 1, 1], initializer=tf.keras.initializers.he_normal(seed=0))\n",
    "W2 = tf.get_variable(\"W2\", [4, 4, 1, 1], initializer=tf.keras.initializers.he_normal(seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92UlJiruxrHU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x00000204DB526828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000204DA3340B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000204DA3340B8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000204DA3340B8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x00000204DA3340B8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFBE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFBE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFBE0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFBE0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFC18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFC18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFC18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000204DA1FFC18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate\n",
    "alpha = 0.0005\n",
    "\n",
    "# First Convolution Network using stride of 1 and padding - same\n",
    "Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "Z1 = tf.layers.batch_normalization(Z1)\n",
    "A1 = tf.nn.relu(Z1)\n",
    "# Maxpooling layer with kernel_size as 2x2 and stride of 1,1\n",
    "P1 = tf.nn.max_pool(A1, ksize = [1, 2, 2, 1], strides = [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Second Convolution Network using stride of 1 and padding - same\n",
    "Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "Z2 = tf.layers.batch_normalization(Z2)\n",
    "A2 = tf.nn.relu(Z2)\n",
    "# Maxpooling layer with kernel_size as 2x2 and stride of 1,1\n",
    "P2 = tf.nn.max_pool(A2, ksize = [1, 2, 2, 1], strides = [1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# Flatten the layers \n",
    "P3 = tf.contrib.layers.flatten(P2)\n",
    "\n",
    "# Fully-connected layers\n",
    "Z3 = tf.contrib.layers.fully_connected(P3, 1024, activation_fn=tf.nn.relu, weights_initializer=tf.keras.initializers.he_normal())\n",
    "#Z3 = tf.nn.dropout(Z3, rate=0.3)\n",
    "Z4 = tf.contrib.layers.fully_connected(Z3, 513, activation_fn=tf.nn.relu, weights_initializer=tf.keras.initializers.he_normal())\n",
    "\n",
    "# Loss function - mean squared error\n",
    "cost = tf.losses.mean_squared_error(predictions = Z4, labels = Y)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=alpha).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mModel Summary:\n",
      "\u001b[0m\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "W1:0 (float32_ref 2x2x1x1) [4, bytes: 16]\n",
      "W2:0 (float32_ref 4x4x1x1) [16, bytes: 64]\n",
      "batch_normalization/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_1/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_1/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "fully_connected/weights:0 (float32_ref 10260x1024) [10506240, bytes: 42024960]\n",
      "fully_connected/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "fully_connected_1/weights:0 (float32_ref 1024x513) [525312, bytes: 2101248]\n",
      "fully_connected_1/biases:0 (float32_ref 513) [513, bytes: 2052]\n",
      "batch_normalization_2/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_2/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_3/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_3/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "fully_connected_2/weights:0 (float32_ref 10260x1024) [10506240, bytes: 42024960]\n",
      "fully_connected_2/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "fully_connected_3/weights:0 (float32_ref 1024x513) [525312, bytes: 2101248]\n",
      "fully_connected_3/biases:0 (float32_ref 513) [513, bytes: 2052]\n",
      "batch_normalization_4/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_4/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_5/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "batch_normalization_5/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "fully_connected_4/weights:0 (float32_ref 10260x1024) [10506240, bytes: 42024960]\n",
      "fully_connected_4/biases:0 (float32_ref 1024) [1024, bytes: 4096]\n",
      "fully_connected_5/weights:0 (float32_ref 1024x513) [525312, bytes: 2101248]\n",
      "fully_connected_5/biases:0 (float32_ref 513) [513, bytes: 2052]\n",
      "Total size of variables: 33099299\n",
      "Total bytes of variables: 132397196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33099299, 132397196)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "print('\\033[1m' + \"Model Summary:\\n\" + '\\033[0m')\n",
    "model_vars = tf.trainable_variables()\n",
    "slim.model_analyzer.analyze_vars(model_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "5zAvl0vRxrHW",
    "outputId": "fe7c302c-3d55-4862-fa9e-c7678e801435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples :  2440\n",
      "Number of Epochs :  500\n",
      "Number of batches :  5\n",
      "Epoch :  0 \tCost :  0.08732897266745568\n",
      "Epoch :  100 \tCost :  0.005619087023660541\n",
      "Epoch :  200 \tCost :  0.004199303640052676\n",
      "Epoch :  300 \tCost :  0.0033429136499762532\n",
      "Epoch :  400 \tCost :  0.002752184332348406\n",
      "Epoch :  499 \tCost :  0.0024205617606639866\n"
     ]
    }
   ],
   "source": [
    "batch_size = 488\n",
    "epochs = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    # Run the initialization\n",
    "    sess.run(init)\n",
    "        \n",
    "    m = len(X_mag_train)\n",
    "    num_batches = int(m/batch_size)\n",
    "    \n",
    "    print(\"Number of training samples : \", m)\n",
    "    print(\"Number of Epochs : \", epochs)\n",
    "    print(\"Number of batches : \", num_batches)\n",
    "    \n",
    "    epoch_loss_ = []\n",
    "    \n",
    "    snr_list = []\n",
    "    snr_list2 = []\n",
    "    \n",
    "    # iterate through epochs\n",
    "    for epoch in range(epochs):\n",
    "        cost_ = 0.0\n",
    "        \n",
    "        # iterate through \n",
    "        i = 0\n",
    "        j = batch_size + 1\n",
    "        for batch in range(num_batches):\n",
    "            X_batch = X_mag_train[i:j]\n",
    "            Y_batch = S_mag_train[i:j]\n",
    "            \n",
    "            i = j\n",
    "            j = j + batch_size\n",
    "            \n",
    "            _,curr_cost = sess.run([optimizer, cost], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            \n",
    "            cost_ = cost_ + curr_cost/num_batches\n",
    "            \n",
    "            y_hat_train = sess.run(Z4, feed_dict={X:X_mag_train})\n",
    "            y_hat_test_01 = sess.run(Z4, feed_dict={X:X_test1})\n",
    "            y_hat_test_02 = sess.run(Z4, feed_dict={X:X_test2})\n",
    "        \n",
    "        snr_list.append(snr(X_mag[19:,:],y_hat_train))       \n",
    "        snr_list2.append(snr(X_test_02_mag[19:,:],y_hat_test_02))\n",
    "        \n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch : \", epoch, \"\\tCost : \", cost_)\n",
    "    print(\"Epoch : \", epoch, \"\\tCost : \", cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSignal to Noise Ratios:\n",
      "\u001b[0m\n",
      "SNR for the original cleaned audio sample (training) and the output of DNN for the training sample. \n",
      "SNR of train_clean_male and y_hat : 16.23022\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m' + \"Signal to Noise Ratios:\\n\" + '\\033[0m')\n",
    "print(\"SNR for the original cleaned audio sample (training) and the output of DNN for the training sample. \")\n",
    "print(\"SNR of train_clean_male and y_hat :\", snr(y_hat_train,S_mag_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0paMzVpxrHY"
   },
   "outputs": [],
   "source": [
    "# Write the outputs to files \n",
    "\n",
    "# Recover the (complex-valued) speech spectrogram of the test signal \n",
    "s_01 = np.multiply(np.divide(X_test_01[:,19:],X_test_01_mag[19:,:].T),y_hat_test_01.T)\n",
    "s_02 = np.multiply(np.divide(X_test_02[:,19:],X_test_02_mag[19:,:].T),y_hat_test_02.T)\n",
    "\n",
    "# Take inverse-STFT \n",
    "out_01 = librosa.istft(s_01, hop_length=512)\n",
    "out_02 = librosa.istft(s_02, hop_length=512)\n",
    "\n",
    "# Write the output\n",
    "librosa.output.write_wav('output2/test_s_01_recons.wav', out_01, sr_01)\n",
    "librosa.output.write_wav('output2/test_s_02_recons.wav', out_02, sr_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
